[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Picasso Monet Painting Classifier\n\n\n\nDeep Learning\n\n\nfastai\n\n\n\nAn image classifier for classifying drawings made by Picasso and Monet.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/painting-classifier/index.html",
    "href": "posts/painting-classifier/index.html",
    "title": "Picasso and Monet Painting Classifier",
    "section": "",
    "text": "Created by Microsoft Designer\nIn this post, let’s build an image classifier to classify paintings made by Picasso and Monet. I am currently doing Practical Deep Learning for Coders course by fastai and this blog post is to summarize my takeways."
  },
  {
    "objectID": "posts/painting-classifier/index.html#install-packages",
    "href": "posts/painting-classifier/index.html#install-packages",
    "title": "Picasso and Monet Painting Classifier",
    "section": "Install packages",
    "text": "Install packages\nLet’s start by installing the required python packages.\n1pip install -Uqq fastai\n2pip install -Uqq duckduckgo_search\n\n1\n\nInstalls fastai library to build and train deep learning models. It also provides utility methods to download images.\n\n2\n\nInstalls duckduckgo_search library to search for images programatically."
  },
  {
    "objectID": "posts/painting-classifier/index.html#import-packages",
    "href": "posts/painting-classifier/index.html#import-packages",
    "title": "Picasso and Monet Painting Classifier",
    "section": "Import packages",
    "text": "Import packages\n\nimport random\nfrom duckduckgo_search import DDGS\nfrom time import sleep\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/painting-classifier/index.html#image-search-using-duckduckgo-search",
    "href": "posts/painting-classifier/index.html#image-search-using-duckduckgo-search",
    "title": "Picasso and Monet Painting Classifier",
    "section": "Image Search using DuckDuckGo Search",
    "text": "Image Search using DuckDuckGo Search\n\nddgs = DDGS()\nsearch_term = 'picasso painting'\nresults = ddgs.images(search_term, max_results=1)\n\nThe ddgs.images(search_term, max_results=1) method uses DuckDuckGo image search engine to search for images with keyword picasso painting and returns the search results. The number of results returned can be controlled by max_results\n\nresults\n\n[{'title': 'Woman s head and self portrait 1929 Picasso - United Kingdom',\n  'image': 'https://cdn11.bigcommerce.com/s-5qm28d53av/images/stencil/2560w/products/330/9318/Woman-s-head-and-self-portrait-1929-Picasso__04078.1586802926.jpg?c=1',\n  'thumbnail': 'https://tse2.mm.bing.net/th?id=OIP.ZCBc3r5pHCQkUXAesuRWtAHaJf&pid=Api',\n  'url': 'https://my-poster.com/Woman-s-head-and-self-portrait-1929-Picasso-/',\n  'height': 3282,\n  'width': 2560,\n  'source': 'Bing'}]\n\n\nThe search results returned by the above method is a list of dictionaries. In the dictionary, the value with key as image is the url containing the image to be downloaded. Let’s use that to download the image using fastai’s download_images method."
  },
  {
    "objectID": "posts/painting-classifier/index.html#download-images",
    "href": "posts/painting-classifier/index.html#download-images",
    "title": "Picasso and Monet Painting Classifier",
    "section": "Download images",
    "text": "Download images\n\nurl = results[0]['image']\n1path = Path('images')\n2path.mkdir(exist_ok=True)\n3download_images(path, urls=[url])\n\n\n1\n\nInitialize Path object to point to the directory for downloading images.\n\n2\n\nCreates a directory pointed by path. exist_ok=True supress the error if directory already exists.\n\n3\n\nDownloads the images from the list of urls passed as urls and saves in the folder pointed by path.\n\n\n\n\nLet’s find the image path and display the image. ls method lists all the contents of the Path object.\n\npath.ls()\n\n(#1) [Path('images/c72d6b91-3379-44f9-8878-92cdf7b87d3c.jpg')]\n\n\n\nim = Image.open('images/c72d6b91-3379-44f9-8878-92cdf7b87d3c.jpg')\n1im.to_thumb(256)\n\n\n1\n\nCreates thumbnail version of the image, no larger than the given size. Here it’s 256px."
  },
  {
    "objectID": "posts/painting-classifier/index.html#download-dataset",
    "href": "posts/painting-classifier/index.html#download-dataset",
    "title": "Picasso and Monet Painting Classifier",
    "section": "Download dataset",
    "text": "Download dataset\nNow let’s wrap the above logics to functions and download the dataset for our Picasso and Monet painting classifier.\n\ndef search_images(search_term: str, max_images:int=30):\n    print(f\"Searching for '{search_term}'\")\n    ddgs = DDGS()\n    results = ddgs.images(search_term, max_results=max_images)\n1    return L(results).itemgot('image')\n\n\n1\n\nextracts all the image attribute from the dictionaries and returns as a list\n\n\n\n\n\n1searches = 'picasso painting', 'monet painting'\n2path = Path('painting')\n\nfor o in searches:\n    dest = path/o\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest,\n                    urls=search_images(f'{o} photo', max_images=100))\n3    sleep(10)\n4    resize_images(path/o, max_size=400, dest=path/o)\n\n\n1\n\nKeywords used to search for images.\n\n2\n\nRoot folder path to save the downloaded images.\n\n3\n\nPause for 10 seconds between each request to the image search API.\n\n4\n\nResize all image files inside path/o to max_size.\n\n\n\n\nSearching for 'picasso painting photo'\nSearching for 'monet painting photo'\n\n\n\nDelete corrupted images\nSometimes the downloaded images may not be in right format or got corrupted. It’s good to find and delete those files before training the model. fastai provides the utility function verify_images which does this for us.\n\n1fns = get_image_files(path)\n2failed = verify_images(fns)\n3failed.map(Path.unlink)\nprint(f\"corrupted images: {len(failed)}\")\n\n\n1\n\nReturns all the image file paths in path directory.\n\n2\n\nVerifies and returns list of image file paths which are corrupted.\n\n3\n\nOn each corrupted path object, apply Path.unlink method which deletes the file.\n\n\n\n\ncorrupted images: 3"
  },
  {
    "objectID": "posts/painting-classifier/index.html#dataloaders",
    "href": "posts/painting-classifier/index.html#dataloaders",
    "title": "Picasso and Monet Painting Classifier",
    "section": "Dataloaders",
    "text": "Dataloaders\nBefore we train the model we should do some datapreprocessing like resizing the images, creating labels for each image file etc., All these can be done through DataBlock and DataLoaders.\n\n1db = DataBlock(\n2    blocks=(ImageBlock, CategoryBlock),\n3    get_items=get_image_files,\n4    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n5    get_y=parent_label,\n6    item_tfms=[Resize(192, method='squish')]\n)\n\n\n1\n\nDataBlock is the class used to write the blueprint of our dataset.\n\n2\n\nWhat are the input (independent variable) and output (dependent variable) data types? ImageBlock tells the input variable in our dataset represents Image datatype and CategoryBlock tells the output variable in our dataset represents Category datatype (classification task).\n\n3\n\nHow do we extract individual items of our dataset? get_image_files is a function which takes root folder path as input and returns all the image files as output.\n\n4\n\nHow to split the dataset into train and validation sets? RandomSplitter is one of the splitting strategies. It splits the dataset randomly. valid_pct=0.2 tells 20% of the dataset should be part of validation set. seed=42 is used for reproducibility.\n\n5\n\nHow to create label (y) for each input sample in our dataset? parent_label is a function which extracts the parent folder name of the file and returns as output.\n\n6\n\nAny transformations to be applied on each item of the dataset? Usually when we train the model, we won’t be training one image at a time but a batch of images. To make this convenient, we make sure all the images are of same size. Resize method is used to resize the image to specified size. Here it’s 192. While resizing, the image may be cropped/squished/anything else. Here, we are asking to squish the image.\n\n\n\n\nThe above code serves as a blueprint of our dataset. But for training the model, we actually need the training and validation dataset created from the downloaded images with all the inputs and outputs in the right format as mentioned in the blueprint. DataLoaders exactly does that.\n\n1dls = db.dataloaders(path, bs=32)\n2dls.train.show_batch(max_n=4, nrows=1)\n\n\n1\n\nCreates dataloaders from the Datablock. Inputs are path and bs. path is used to point to the downloaded dataset path from which all the files are read by the get_image_files function used in datablock definition. bs is the batch size which ensures during model training the model gets bs # of images at a time rather just 1. This is done to make efficient use of GPUs as well as other advantages.\n\n2\n\nOnce the dataloaders is created, we can access the training and validation set using train and valid attributes respectively. The dls.train.show_batch displays 4 images from the training set in a single row which is set by max_n and nrows."
  },
  {
    "objectID": "posts/painting-classifier/index.html#training",
    "href": "posts/painting-classifier/index.html#training",
    "title": "Picasso and Monet Painting Classifier",
    "section": "Training",
    "text": "Training\nNow we have the dataset ready, let’s train the model to classify Picasso and Monet images. For this project, rather than training a deep learning model from scratch let’s use a pretrained model called resnet18 and finetune it for the task (Picasso and Monet classification) at hand.\nIn fastai, most of the trainer has a common api which is domain_learner. Since we are going to solve a Computer Vision problem, we’ll be using vision_learner. We will be passing the dataset, pretrained deep learning model and metrics (to log) as inputs. We’ll train the model using finetune method.\n\n1learn = vision_learner(dls, resnet18, metrics=error_rate)\n2learn.fine_tune(5)\n\n\n1\n\ndls is the dataloaders we created in previous setps. resnet18 is the Residual Network architecture with 18 layers pre-trained on Imagenet dataset. error_rate is the percentage of validation examples the model predicts wrong.\n\n2\n\nfine_tune finetunes the model for the new dataset at hand for given number of epochs. It also prints the training progress.\n\n\n\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 144MB/s] \n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.467476\n0.218055\n0.054795\n00:03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.083487\n0.011494\n0.000000\n00:01\n\n\n1\n0.076097\n0.001811\n0.000000\n00:01\n\n\n2\n0.058125\n0.001642\n0.000000\n00:01\n\n\n3\n0.043209\n0.002014\n0.000000\n00:01\n\n\n4\n0.032918\n0.002884\n0.000000\n00:01\n\n\n\n\n\n\nWe can see that the validation error is around 0. It means the model does pretty good job in differentiating Picasso and Monet paintings.\n\n\n\n\n\n\nNote\n\n\n\nThe original model was pretrained on a different dataset for a different problem. For our project, we need to predict which class a painting belongs to. We have two classes. So the output layer of the model should be replaced with two nodes. This is automatically taken care by fastai. The number 5 represents the number of epochs the model should be trained for. When a model sees all the training sample once, we call it an epoch.\nThe finetune method also employes best practices while training the model. We can see there are two sections in the training log. The second section represents the 5 epochs which we requested fastai to train for. In those 5 epochs, all the layers are trained and the metrics are logged. So then what is the first section of log which shows additional epoch? The first section shows the training log of one epoch done by fastai by freezing (no training) all the layers expect the last layer which was inserted newly for our task.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAn epoch is nothing but single pass of all the samples in the training dataset by the model."
  },
  {
    "objectID": "posts/painting-classifier/index.html#prediction",
    "href": "posts/painting-classifier/index.html#prediction",
    "title": "Picasso and Monet Painting Classifier",
    "section": "Prediction",
    "text": "Prediction\nNow we have our trained model, let’s make a prediction using the predict method.\n\nfns = get_image_files(Path('painting'))\n\n\n1fn = random.choice(fns)\n2preds = learn.predict(fn)\nprint(preds)\n\nImage.open(fn)\n\n\n1\n\nRandomly chooses an image file from the list of images.\n\n2\n\npredict method is used to make a prediction using the trained model learn.\n\n\n\n\n\n\n\n\n\n\n\n('monet painting', tensor(0), tensor([1.0000e+00, 1.8999e-08]))\n\n\n\n\n\n\n\n\n\nThe model predicts the output as ‘monet painting’ and it’s 100% confident about it!\nThe predicted output is a tuple containing three values. They are (predicted label, predicted label as index, class probabilities for each class). The index corresponding to maximum class probability is chosen as the predicted class and corresponding index is returned in the second value of the tuple. Now we have the class index, how to find the class label? learn.dls.vocab or dls.vocab can be used to see the list of labels and the order used by fastai.\n\nlearn.dls.vocab\n\n['monet painting', 'picasso painting']\n\n\nHere, the \\(0^{th}\\) index represents ‘monet painting’. Hence, the first value of the tuple returned by the predict method has ‘monet painting’ in it."
  },
  {
    "objectID": "posts/painting-classifier/index.html#save-the-model",
    "href": "posts/painting-classifier/index.html#save-the-model",
    "title": "Picasso and Monet Painting Classifier",
    "section": "Save the model",
    "text": "Save the model\nNow we have our trained model, we can use this in other apps. To do so, first we need to save the model so we need not retrain. In fastai, the model can be saved to a file using the export method.\n\n1learn.export('model.pkl')\n\n\n1\n\nExport (save) the trained model in the given path.\n\n\n\n\nI deployed the above model as a web app where a user can upload images of Picasso and Monet paintings and get predictions. You can find it here. In future posts, I will share how I did it.\n\n\n\nApp deployed in HuggingFace Spaces\n\n\nThat’s it for today. See you in another post."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Harish",
    "section": "",
    "text": "I’m a Software Engineer and Machine Learning student from India."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Harish",
    "section": "",
    "text": "I’m a software engineer and a machine learning student. This is where I share lessons learned during my projects, spanning Python programming, Deep Learning, and Software Design.\n\nRecent posts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFastbook NLP Deep Dive: RNNs Q&A\n\n\n\n\n\n\nDeep Learning\n\n\nfastai\n\n\nNLP\n\n\nfastbook\n\n\n\n\n\n\n\n\n\nApr 26, 2024\n\n\nHarish\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Deep Learning Model\n\n\n\n\n\n\nDeep Learning\n\n\nHugging Face\n\n\nfastai\n\n\nModel Deployment\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\nHarish\n\n\n\n\n\n\n\n\n\n\n\n\nPicasso and Monet Painting Classifier\n\n\n\n\n\n\nDeep Learning\n\n\nfastai\n\n\n\n\n\n\n\n\n\nMar 31, 2024\n\n\nHarish\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nwelcome\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\nHarish\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\n\nCreated by Microsoft Designer\n\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "Harish B",
    "section": "",
    "text": "Hugging Face + Gradio Model Deployment\nLanguage Model Fine-Tuning & Text Classification\nMetric vs Loss\nUnderfit vs Overfit\nWeb API\nDjango\nCORS\nCSRF"
  },
  {
    "objectID": "posts/model-deploy-hfspace/index.html",
    "href": "posts/model-deploy-hfspace/index.html",
    "title": "Deploy Deep Learning Model",
    "section": "",
    "text": "In the previous post, we saw how to train a model (to classify Picasso and Monet paintings) and save the trained model in fastai. In this post, let us create an app using gradio that uses the trained model and we will also learn how to deploy the app using Hugging Face Spaces so that others can use our app. Incase if you are not familiar with training and saving a model, I suggest you to go through Picasso and Monet Painting Classifier post."
  },
  {
    "objectID": "posts/model-deploy-hfspace/index.html#import-packages",
    "href": "posts/model-deploy-hfspace/index.html#import-packages",
    "title": "Deploy Deep Learning Model",
    "section": "Import packages",
    "text": "Import packages\nTo load the model, we need to import necessary modules from fastai. fastai.vision.all module provides functions related to computer vision tasks.\n\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/model-deploy-hfspace/index.html#load-the-model",
    "href": "posts/model-deploy-hfspace/index.html#load-the-model",
    "title": "Deploy Deep Learning Model",
    "section": "Load the model",
    "text": "Load the model\n\n1learn = load_learner('model.pkl')\n\n\n1\n\nLoads the trained model saved as model.pkl file.\n\n\n\n\nNotImplementedError: cannot instantiate 'PosixPath' on your system\nUnfortunately when you load the model in a Windows system, it throws above error. Because in my case, I trained and saved the model in a Linux system and trying to load it in Windows. To get rid of this error, below code should be run before loading the model.\n\nimport platform\n1if platform.system() == 'Windows':\n    import pathlib\n    pathlib.PosixPath = pathlib.WindowsPath\n\n\n1\n\nChecks if the OS where the code is run is Windows?\n\n\n\n\n\nlearn = load_learner('model.pkl')\n\nNow the model is loaded successfully 🙂! Before making the predictions, let’s see the possible predictions (labels) of the model.\n\n1labels = learn.dls.vocab\nlabels\n\n\n1\n\ndls refers to DataLoaders and vocab refers to the class labels.\n\n\n\n\n['monet painting', 'picasso painting']\n\n\nGiven an image as input our model will predict either monet painting or picasso painting."
  },
  {
    "objectID": "posts/model-deploy-hfspace/index.html#predict",
    "href": "posts/model-deploy-hfspace/index.html#predict",
    "title": "Deploy Deep Learning Model",
    "section": "Predict",
    "text": "Predict\nLet’s read an image and make prediction with the loaded model.\n\n1img = PILImage.create('images/picas3.jpg')\n2img.to_thumb(256)\n\n\n1\n\nRead the image from the path images/picas3.jpg.\n\n2\n\nResize the image to a thumbnail of size 256.\n\n\n\n\n\n\n\n\n\n\n\nNow pass the loaded image to the model’s predict method which returns the class label.\n\npred_label, pred_index, class_probs = learn.predict(img)\nprint(\"Prediction:\", pred_label)\nprint(\"Class probabilities:\", class_probs)\nprint(\"Prediction Index:\", pred_index)\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\nPrediction: picasso painting\nClass probabilities: tensor([0.0028, 0.9972])\nPrediction Index: tensor(1)\n\n\nWe could see the model predicting it’s a Picasso’s painting and it is 99% confident about it. We can use the prediction index in the class probabilities array to get the probability (confidence) score of the predicted label.\nLet’s wrap the above logic to a function which gets image path as input and returns a dictionary with class label and confidence score as output.\n\ndef predict(img_path):\n1    img = PILImage.create(img_path)\n2    pred_label, pred_index, class_probs = learn.predict(img)\n3    output = {labels[i]:float(class_probs[i]) for i in range(len(labels))}\n    return output\n\n\n1\n\nRead image in the img_path.\n\n2\n\nMake predictions for the image.\n\n3\n\nCreate output dictionary where key is the class label and value is the probability score. The output is specifically constructed like this because gradio expects the labels to be in this format. We’ll discuss about gradio in the next section.\n\n\n\n\nNow let’s use the predict function to make predictions for few images.\n\npredict('images/picas1.jpg')\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n{'monet painting': 1.7153462977148592e-05,\n 'picasso painting': 0.9999828338623047}\n\n\n\npredict('images/mon2.jpg')\n\n\n\n\n\n\n\n\n{'monet painting': 0.999991774559021,\n 'picasso painting': 8.193430403480306e-06}\n\n\nNow we successfully loaded the trained model and made predictions, let’s develop an app and use this model in it."
  },
  {
    "objectID": "posts/model-deploy-hfspace/index.html#hugging-face-spaces",
    "href": "posts/model-deploy-hfspace/index.html#hugging-face-spaces",
    "title": "Deploy Deep Learning Model",
    "section": "Hugging Face Spaces",
    "text": "Hugging Face Spaces\nLet’s create a space for our gradio app. Once you log into your Hugging Face account, go to Hugging Face Spaces and click Create new space.\n\n\n\n\n\nYou will be landed to a form where we will be giving basic details like the space name, license, app framework, what kind of server configs we need etc., Hugging Face provides variety of hardware like CPUs and GPUs. For our demo, we will choose the free CPU.\n\n\n\n\n\n\n\n\n\n\nOnce you click Create Space, Hugging Face will create a git repo for you which we need to clone locally and update them with trained model and python scripts. Then the updated local repo should be pushed to Hugging Face remote repo.\nLet’s clone the repo as per the instruction given.\ngit clone https://huggingface.co/spaces/harishb00/painting-classifier"
  },
  {
    "objectID": "posts/model-deploy-hfspace/index.html#convert-jupyter-code-to-python-script",
    "href": "posts/model-deploy-hfspace/index.html#convert-jupyter-code-to-python-script",
    "title": "Deploy Deep Learning Model",
    "section": "Convert Jupyter code to python script",
    "text": "Convert Jupyter code to python script\nConvert all the code we written in Jupyter notebook to Python script and save it to app.py file in the cloned repo. Once we do it, it looks like below\n\n\napp.py\n\nimport platform\nfrom fastai.vision.all import *\nimport gradio as gr\n\n\nif platform.system() == 'Windows':\n    import pathlib\n    pathlib.PosixPath = pathlib.WindowsPath\n\nlearn = load_learner('model.pkl')\nlabels = learn.dls.vocab\n\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn.predict(img)\n    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n\ntitle = \"Painting Classifier\"\ndescription = \"A painting classifier trained to classify Picasso and Monet paintings using dataset scrapped from Duck Duck Go Image search.\"\ndemo = gr.Interface(\n    inputs=gr.Image(height=400),\n    outputs=gr.Label(),\n    fn=predict,\n    title=title,\n    description=description,\n    examples=['images/picas3.jpg', 'images/picas1.jpg'],\n)\ndemo.launch()\n\nAlso create a requirements file inside cloned repo with Python packages required to run the script.\n\n\nrequirements.txt\n\nfastai\ngradio\n\nCopy the model file and image example files to the same folder. Once you are done with all the steps, your folder structure looks like this."
  },
  {
    "objectID": "posts/model-deploy-hfspace/index.html#commit-and-push",
    "href": "posts/model-deploy-hfspace/index.html#commit-and-push",
    "title": "Deploy Deep Learning Model",
    "section": "Commit and Push",
    "text": "Commit and Push\nIt’s time to commit and push the code to Hugging Face Spaces. Execute below commands in your terminal.\ngit add .\ngit commit -m \"app commit\"\ngit push\n\n\n\n\n\n\ngit push hangs\n\n\n\ngit push hangs at 74% for me since the uploaded model file is a large one. git config --global http.postBuffer 157286400 fixed the issue by increasing the git buffer.\n\n\nThat’s it. In few minutes, your app is live and you can visit the Hugging Face Spaces project URL (URL used with git clone URL) to use/share your app."
  },
  {
    "objectID": "posts/fastbook-ch10-qa/index.html",
    "href": "posts/fastbook-ch10-qa/index.html",
    "title": "Fastbook NLP Deep Dive: RNNs Q&A",
    "section": "",
    "text": "In this post, I tried to answer all the questions in Chapter 10 of Fastbook which is NLP Deep Dive: RNNs.\n\n1. What is “self-supervised learning”?\nTraining a model using labels that are embedded in the independent variable, rather than requiring external labels. For example, training a language model to predict the next word in a text.\n\n\n2. What is a “language model”?\nLanguage model is a model that has been trained to guess what the next word of a given passage is.\n\n\n3. Why is a language model considered self-supervised?\nThe language model is considered self-supervised because there are no labels provided during training. The model learns to predict the next word by reading lots of texts.\n\n\n4. What are self-supervised models usually used for?\n\nProblems where labeled data is not adequate\nLanguage models\nPre-training models for transfer learning\n\n\n\n5. Why do we fine-tune language models?\nLanguage models might be trained on a corpus that is different than the task at hand. Fine-tuning them helps the model to be good at task specific corpus.\n\n\n6. What are the three steps to create a state-of-the-art text classifier?\n\nTrain a language model on a big corpus of text like wikipedia or use a pre-trained language model.\nFine-tuning the language model on text classification dataset.\nFine-tune the classifier model given the language model as the encoder.\n\n\n\n7. How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\nWe can use the unlabelled movie reviews to fine-tune the language model so that it understands the language of movie reviews. This requires no labelling. Now we can use this fine-tuned language model (that knows how to predict next word in a movie review!) as a base for our text classifier.\n\n\n8. What are the three steps to prepare your data for a language model?\n\nTokenization: Text to tokens\nNumericalization: tokens to integers\nBatches: Stream of documents to a batch of fixed-size input and outputs (tokens offset by one token). Taken care by LMDataLoader.\n\n\n\n9. What is “tokenization”? Why do we need it?\nTokenization is the process of converting raw text to list of tokens (words, characters, or substrings, depending on the granularity of the model). It enables us to represent each token numerically which the models can understand (compared to text).\n\n\n10. Name three different approaches to tokenization.\n\nWord-based: Split a sentence on spaces (or language-specific rules that define what a word is.) Eg: don’t -&gt; do, n’t\nSubword-based: Split words into smaller parts, based on the most commonly occuring substrings.\nCharacter-based: Split a sentence into its individual characters.\n\n\n\n11. What is xxbos?\nxxbos is a special token added by fastai tokenizer that indicates beginning of the stream (text). With this, the model will be able to learn it needs to forget what we said previously and focus on upcoming words.\n\n\n12. List four rules that fastai applies to text during tokenization.\n\nreplace_wrep: Replaces any word repeated three times or more with a special token for word repetion (xxwrep), the number of times it’s repeated, then the word.\nrm_useless_spaces: Removes all repetitions of the space character.\nreplace_maj: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it.\nlowercase: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)\n\n\nfrom fastai.text.core import lowercase\nfrom fastai.text.core import replace_wrep\nfrom fastai.text.core import rm_useless_spaces\nfrom fastai.text.core import replace_maj\n\n\nlowercase('My name is Harish.')\n\n'xxbos my name is harish.'\n\n\n\nreplace_maj(\"My name is Harish.\")\n\n'xxmaj my name is xxmaj harish.'\n\n\n\nrm_useless_spaces(\"My    name  is      Harish.\")\n\n'My name is Harish.'\n\n\n\nreplace_wrep(\"My name is harish harish harish\")\n\n'My name is xxwrep 3 harish '\n\n\n\n\n13. Why are repeated characters replaced with a token showing the number of repetitions and the character that’s repeated?\nIn this way, the model’s embedding matrix can encode information about general concepts such as repeated punctuation (or any character) rather than requiring a seperate token for every number of repetitions.\n\n\n14. What is “numericalization”?\nNumericalization is the process of mapping tokens to integers.\n\n\n15. Why might there be words that are replaced with the “unknown word” token?\nWe’ll be having lot of rare words in the corpus for which there won’t be enough data to train representations for the same. These words can be replaced with xxunk token. This is useful to avoid having an overly large embedding matrix with lot of rare words, since that can slow down training and use up too much memory.\n\n\n16. With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book’s website.)\n\n\\(Batch\\ Size = 64\\). The dataset is split into \\(64\\) stream of texts.\n\\(Sequence\\ Length = 64\\). Each row (\\(i\\)) in an individual batch has \\(64\\) tokens from the \\(i^{th}\\) stream of text.\n\\(1^{st}\\) row of \\(1^{st}\\) batch contains 64 tokens from \\(1^{st}\\) stream of text starting from \\(0^{th}\\) token.\n\\(2^{nd}\\) row of \\(1^{st}\\) batch contains 64 tokens from \\(2^{nd}\\) stream of text starting from \\(0^{th}\\) token.\n\\(1^{st}\\) row of \\(2^{nd}\\) batch contains 64 tokens from \\(1^{st}\\) stream of text starting from \\(65^{th}\\) token.\n\n\n\n17. Why do we need padding for text classification? Why don’t we need it for language modeling?\nPyTorch DataLoaders need to collate all the items into a single tensor, and a single tensor has a fixed shape. We can’t do cropping like we do for Images to bring all inputs to a fixed size. So we do padding. We use a special padding token that will be ignored by our model.\nHow?\nWe won’t pad every batch to the same size, but will instead use the size of the largest document in each batch as the target size. Additionally, to avoid memory issues and improve performance, we will batch together texts that are roughly the same lengths (with some shuffling for the training set). We do this by (approximately, for the training set) sorting the documents by length prior to each epoch. The result of this is that the documents collated into a single batch will tend to be of similar lengths.\nWhy don’t we need padding for language modelling?\nIn language modelling, the input to the model is a big corpus of text concatenated to a single stream. We don’t have explicit notion of first sample, second sample where each sample is of different size. Every row in an individual batch is a part (fixed sequence length in order) of the big corpus.\n\n\n18. What does an embedding matrix for NLP contain? What is its shape?\nEmbedding matrix contain the vector representation of vocab in the corpus. It encodes token to a vector representation. It’s a matrix with |vocab| number of rows and x columns. Here x is 400 for the model used in the chapter. It can vary depending on how the embedding layers are defined.\n\n\n19. What is “perplexity”?\nPerplexity is exponential of the loss function used in language model. torch.exp(cross_entropy)\nIn general, perplexity is a measurement of how well a probability model predicts a sample. It quantifies how uncertain the model is in predicting the next word in a sequence. Lower perplexity values indicate better performance, as the model is less “perplexed” (puzzled) by the data.\n\n\n20. Why do we have to pass the vocabulary of the language model to the classifier data block?\nThe reason that we pass the vocab of the language model is to make sure we use the same correspondence of token to index. Otherwise the embeddings we learned in our fine-tuned language model won’t make any sense to this model, and the fine-tuning step won’t be of any use.\ndls_clas = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n\n\n21. What is “gradual unfreezing”?\nIt’s a technique in fine-tuning where we unfreeze few layers at a time until the whole model is unfroze. In NLP classifiers, it makes a real difference and achieved using learn.freeze_to method.\nlearn.fit_one_cycle(1, 2e-2) # by default only last layer unfroze for pretrained models\nlearn.freeze_to(-2) # unfreeze last two param groups\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\nlearn.freeze_to(-3) # unfreeze last three param groups\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\nlearn.unfreeze() # unfreeze all layers\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\nHere discriminative learning rates are used.\n\n\n22. Why is text generation always likely to be ahead of automatic identification of machine-generated texts?\nClassification algorithms can automatically recognise autogenerated content. The problem, however, is that this will always be an arms race, in which better classification (or discriminator) algorithms can be used to create better generation algorithms."
  }
]